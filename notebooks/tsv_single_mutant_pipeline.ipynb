{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TSV-driven single-mutant Kermut pipeline\n",
        "\n",
        "This notebook ingests a TSV file describing variant measurements, filters for single substitutions, prepares the Kermut-compatible data layout, and evaluates Kermut with three-fold cross-validation. Run it inside the Kermut repository after activating the project environment so that Hydra configs and preprocessing utilities are available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook outline\n",
        "\n",
        "1. Configure inputs, objectives, and feature toggles.\n",
        "2. Parse the TSV, construct mutated sequences, and aggregate multi-objective targets.\n",
        "3. Save assay tables plus a reference manifest in the layout expected by Kermut.\n",
        "4. Inspect skipped records and verify that required embeddings/structure features exist.\n",
        "5. Compose a Hydra config and run 3-fold train/test splits to obtain performance metrics.\n",
        "6. Export predictions, per-fold metrics, and aggregated summaries for downstream analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from IPython.display import display\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "\n",
        "from kermut.data import prepare_GP_inputs, prepare_GP_kwargs, split_inputs, standardize\n",
        "from kermut.gp import instantiate_gp, optimize_gp, predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure input paths and runtime options\n",
        "\n",
        "Update the paths in the next cell to point to your TSV file and any directories where intermediate outputs should be written. The workspace folder will store generated assay CSVs, Hydra reference manifests, cross-validation predictions, and metric summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Update these values before running the pipeline ---\n",
        "\n",
        "tsv_path = Path(\"path/to/your_assay.tsv\")  # <-- replace with the TSV you want to process\n",
        "\n",
        "workspace_root = Path(\"data/notebook_runs/example_run\")\n",
        "assay_output_dir = workspace_root / \"assays\"\n",
        "reference_manifest_path = workspace_root / \"reference.csv\"\n",
        "cv_predictions_dir = workspace_root / \"cv_predictions\"\n",
        "\n",
        "feature_paths: Dict[str, Optional[Path]] = {\n",
        "    \"embeddings\": Path(\"data/embeddings/substitutions_singles/ESM2\"),\n",
        "    \"zero_shot\": Path(\"data/zero_shot_fitness_predictions\"),\n",
        "    \"conditional_probs\": Path(\"data/conditional_probs/ProteinMPNN\"),\n",
        "    \"coords\": Path(\"data/structures/coords\"),\n",
        "}\n",
        "\n",
        "kernel_config_name = \"kermut\"  # pick from kermut/hydra_configs/kernel/*.yaml\n",
        "DATA_STANDARDIZE = True\n",
        "kernel_overrides: Dict[str, object] = {\n",
        "    \"use_zero_shot\": False,\n",
        "    # Example: \"structure_kernel.use_distance_comparison\": False,\n",
        "}\n",
        "\n",
        "optimization_overrides: Dict[str, object] = {\n",
        "    \"n_steps\": 150,\n",
        "    \"lr\": 0.1,\n",
        "    \"progress_bar\": False,\n",
        "}\n",
        "\n",
        "FOLD_COLUMN = \"cv_fold_3\"\n",
        "CV_FOLDS = 3\n",
        "RANDOM_SEED = 2024\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "\n",
        "for path in [workspace_root, assay_output_dir, cv_predictions_dir]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure optimisation objectives\n",
        "\n",
        "Describe each target column that should contribute to the aggregate `DMS_score`. Provide the TSV column name, a weight (the code will normalise weights to sum to 1), and whether the objective should be maximised or minimised."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example configuration:\n",
        "# objectives = [\n",
        "#     {\"column\": \"fitness\", \"weight\": 0.7, \"goal\": \"maximize\"},\n",
        "#     {\"column\": \"stability\", \"weight\": 0.3, \"goal\": \"minimize\"},\n",
        "# ]\n",
        "\n",
        "objectives: List[Dict[str, object]] = []\n",
        "OBJECTIVE_NORMALIZATION = \"zscore\"  # one of {\"zscore\", \"minmax\", \"none\"}\n",
        "DROP_ROWS_WITH_MISSING_OBJECTIVES = True\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper functions\n",
        "\n",
        "The utilities below handle mutation parsing, multi-objective aggregation, fold assignment, Hydra configuration, and metric summarisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SINGLE_MUTATION_RE = re.compile(r\"^(?P<wt>[A-Z])(?P<pos>\\d+)(?P<mut>[A-Z])$\")\n",
        "NORMALIZATION_METHODS = {\"zscore\", \"minmax\", \"none\"}\n",
        "\n",
        "def validate_objective_config(obj_list: List[Dict[str, object]]) -> pd.DataFrame:\n",
        "    if not obj_list:\n",
        "        raise ValueError(\n",
        "            \"No objectives configured. Update the `objectives` list in the configuration cell.\"\n",
        "        )\n",
        "    df_obj = pd.DataFrame(obj_list)\n",
        "    required_keys = {\"column\", \"weight\", \"goal\"}\n",
        "    missing_keys = required_keys - set(df_obj.columns)\n",
        "    if missing_keys:\n",
        "        raise ValueError(f\"Objective definitions are missing keys: {sorted(missing_keys)}\")\n",
        "    df_obj[\"column\"] = df_obj[\"column\"].astype(str)\n",
        "    df_obj[\"weight\"] = df_obj[\"weight\"].astype(float)\n",
        "    if (df_obj[\"weight\"] < 0).any():\n",
        "        raise ValueError(\"Objective weights must be non-negative.\")\n",
        "    weight_sum = df_obj[\"weight\"].sum()\n",
        "    if weight_sum == 0:\n",
        "        raise ValueError(\"Objective weights must sum to a positive value.\")\n",
        "    df_obj[\"weight\"] = df_obj[\"weight\"] / weight_sum\n",
        "    df_obj[\"goal\"] = df_obj[\"goal\"].str.lower()\n",
        "    allowed_goals = {\"maximize\", \"minimize\"}\n",
        "    invalid_goals = set(df_obj[\"goal\"]) - allowed_goals\n",
        "    if invalid_goals:\n",
        "        raise ValueError(\n",
        "            f\"Objective goals must be 'maximize' or 'minimize', got {sorted(invalid_goals)}.\"\n",
        "        )\n",
        "    return df_obj\n",
        "\n",
        "\n",
        "def ensure_columns(df: pd.DataFrame, required: Iterable[str]) -> None:\n",
        "    missing = [col for col in required if col not in df.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing required columns in TSV: {missing}\")\n",
        "\n",
        "\n",
        "def parse_mutation_tokens(value) -> List[str]:\n",
        "    if pd.isna(value):\n",
        "        return []\n",
        "    tokens = re.split(r\"[;,\\s]+\", str(value).strip())\n",
        "    return [token.strip().upper() for token in tokens if token.strip()]\n",
        "\n",
        "\n",
        "def apply_single_mutation(sequence: str, mutation: str) -> Tuple[str, str, str, int]:\n",
        "    match = SINGLE_MUTATION_RE.match(mutation)\n",
        "    if match is None:\n",
        "        raise ValueError(f\"Mutation '{mutation}' is not a single amino-acid substitution.\")\n",
        "    wt = match.group(\"wt\")\n",
        "    mut = match.group(\"mut\")\n",
        "    pos = int(match.group(\"pos\"))\n",
        "    if not isinstance(sequence, str) or not sequence:\n",
        "        raise ValueError(\"Missing reference sequence.\")\n",
        "    if pos < 1 or pos > len(sequence):\n",
        "        raise ValueError(f\"Position {pos} out of bounds for sequence of length {len(sequence)}.\")\n",
        "    if sequence[pos - 1] != wt:\n",
        "        raise ValueError(\n",
        "            f\"Expected residue '{wt}' at position {pos}, but sequence contains '{sequence[pos - 1]}'.\"\n",
        "        )\n",
        "    mutated_sequence = sequence[: pos - 1] + mut + sequence[pos:]\n",
        "    return mutated_sequence, wt, mut, pos\n",
        "\n",
        "\n",
        "def extract_single_mutants(\n",
        "    df_raw: pd.DataFrame, objectives_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    records: List[Dict[str, object]] = []\n",
        "    skipped: List[Dict[str, object]] = []\n",
        "    objective_cols = objectives_df[\"column\"].tolist()\n",
        "    for idx, row in df_raw.iterrows():\n",
        "        reference_key = row[\"reference_key\"]\n",
        "        mutation_tokens = parse_mutation_tokens(row[\"mutations\"])\n",
        "        single_tokens = [token for token in mutation_tokens if SINGLE_MUTATION_RE.match(token)]\n",
        "        if len(single_tokens) != 1:\n",
        "            reason = \"no_single_mutation\" if len(single_tokens) == 0 else \"multi_mutation\"\n",
        "            skipped.append(\n",
        "                {\n",
        "                    \"row_index\": idx,\n",
        "                    \"reference_key\": reference_key,\n",
        "                    \"reason\": reason,\n",
        "                    \"raw_mutations\": row.get(\"mutations\", \"\"),\n",
        "                }\n",
        "            )\n",
        "            continue\n",
        "        mutation = single_tokens[0]\n",
        "        try:\n",
        "            mutated_sequence, wt, mut, pos = apply_single_mutation(row[\"sequence\"], mutation)\n",
        "        except ValueError as exc:\n",
        "            skipped.append(\n",
        "                {\n",
        "                    \"row_index\": idx,\n",
        "                    \"reference_key\": reference_key,\n",
        "                    \"reason\": str(exc),\n",
        "                    \"raw_mutations\": row.get(\"mutations\", \"\"),\n",
        "                }\n",
        "            )\n",
        "            continue\n",
        "        record: Dict[str, object] = {\n",
        "            \"reference_key\": reference_key,\n",
        "            \"mutant\": mutation,\n",
        "            \"mutation\": mutation,\n",
        "            \"wt_aa\": wt,\n",
        "            \"mut_aa\": mut,\n",
        "            \"position\": pos,\n",
        "            \"sequence\": row[\"sequence\"],\n",
        "            \"mutated_sequence\": mutated_sequence,\n",
        "            \"backbone_id\": row.get(\"backbone_id\"),\n",
        "            \"pdb\": row.get(\"pdb\"),\n",
        "        }\n",
        "        for col in objective_cols:\n",
        "            record[col] = row.get(col)\n",
        "        records.append(record)\n",
        "    df_single = pd.DataFrame(records)\n",
        "    skip_df = pd.DataFrame(skipped)\n",
        "    return df_single, skip_df\n",
        "\n",
        "\n",
        "def drop_rows_with_missing_objectives(\n",
        "    df: pd.DataFrame, objective_cols: List[str]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    if df.empty:\n",
        "        return df.copy(), pd.DataFrame(columns=df.columns)\n",
        "    missing_mask = df[objective_cols].isnull().any(axis=1)\n",
        "    dropped = df.loc[missing_mask].copy()\n",
        "    kept = df.loc[~missing_mask].copy()\n",
        "    return kept.reset_index(drop=True), dropped.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def normalize_objectives(\n",
        "    df: pd.DataFrame, objectives_df: pd.DataFrame, method: str\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Dict[str, float]]]:\n",
        "    method = method.lower()\n",
        "    if method not in NORMALIZATION_METHODS:\n",
        "        raise ValueError(\n",
        "            f\"Unknown normalization method '{method}'. Valid options: {sorted(NORMALIZATION_METHODS)}.\"\n",
        "        )\n",
        "    df_norm = df.copy()\n",
        "    stats: Dict[str, Dict[str, float]] = {}\n",
        "    for _, obj in objectives_df.iterrows():\n",
        "        col = obj[\"column\"]\n",
        "        values = df_norm[col].astype(float)\n",
        "        if method == \"zscore\":\n",
        "            mean = values.mean()\n",
        "            std = values.std(ddof=0)\n",
        "            if std == 0:\n",
        "                normalized = values - mean\n",
        "            else:\n",
        "                normalized = (values - mean) / std\n",
        "            stats[col] = {\"mean\": float(mean), \"std\": float(std)}\n",
        "        elif method == \"minmax\":\n",
        "            min_val = values.min()\n",
        "            max_val = values.max()\n",
        "            if max_val == min_val:\n",
        "                normalized = values - min_val\n",
        "            else:\n",
        "                normalized = (values - min_val) / (max_val - min_val)\n",
        "            stats[col] = {\"min\": float(min_val), \"max\": float(max_val)}\n",
        "        else:\n",
        "            normalized = values\n",
        "            stats[col] = {}\n",
        "        df_norm[f\"{col}__normalized\"] = normalized\n",
        "    return df_norm, stats\n",
        "\n",
        "\n",
        "def compute_weighted_scores(df: pd.DataFrame, objectives_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_scores = df.copy()\n",
        "    contribution_cols: List[str] = []\n",
        "    for _, obj in objectives_df.iterrows():\n",
        "        col = obj[\"column\"]\n",
        "        norm_col = f\"{col}__normalized\"\n",
        "        signed_col = f\"{col}__signed\"\n",
        "        weight_col = f\"{col}__weighted\"\n",
        "        sign = 1.0 if obj[\"goal\"] == \"maximize\" else -1.0\n",
        "        df_scores[signed_col] = df_scores[norm_col] * sign\n",
        "        df_scores[weight_col] = df_scores[signed_col] * obj[\"weight\"]\n",
        "        contribution_cols.append(weight_col)\n",
        "    df_scores[\"DMS_score\"] = df_scores[contribution_cols].sum(axis=1)\n",
        "    return df_scores\n",
        "\n",
        "\n",
        "def assign_cv_folds(\n",
        "    df: pd.DataFrame, fold_column: str, n_splits: int, seed: int\n",
        ") -> pd.DataFrame:\n",
        "    if n_splits < 2:\n",
        "        raise ValueError(\"n_splits must be >= 2.\")\n",
        "    if len(df) < n_splits:\n",
        "        raise ValueError(\n",
        "            f\"Cannot create {n_splits} folds because the assay only contains {len(df)} variants.\"\n",
        "        )\n",
        "    rng = np.random.default_rng(seed)\n",
        "    indices = np.arange(len(df))\n",
        "    rng.shuffle(indices)\n",
        "    fold_indices = np.array_split(indices, n_splits)\n",
        "    assignments = np.empty(len(df), dtype=int)\n",
        "    for fold_idx, fold_idx_values in enumerate(fold_indices):\n",
        "        assignments[fold_idx_values] = fold_idx\n",
        "    df_with_folds = df.copy()\n",
        "    df_with_folds[fold_column] = assignments\n",
        "    return df_with_folds\n",
        "\n",
        "\n",
        "def build_reference_table(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    reference_cols = [\"reference_key\", \"sequence\", \"backbone_id\", \"pdb\"]\n",
        "    ensure_columns(df, reference_cols)\n",
        "    reference_df = (\n",
        "        df[reference_cols]\n",
        "        .drop_duplicates(subset=[\"reference_key\"])\n",
        "        .rename(\n",
        "            columns={\n",
        "                \"reference_key\": \"DMS_id\",\n",
        "                \"sequence\": \"target_seq\",\n",
        "                \"pdb\": \"pdb_path\",\n",
        "            }\n",
        "        )\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return reference_df\n",
        "\n",
        "\n",
        "def compute_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    if y_true.size == 0:\n",
        "        return {\"spearman\": np.nan, \"pearson\": np.nan, \"rmse\": np.nan, \"mae\": np.nan}\n",
        "    true_series = pd.Series(y_true)\n",
        "    pred_series = pd.Series(y_pred)\n",
        "    metrics = {\n",
        "        \"spearman\": float(true_series.corr(pred_series, method=\"spearman\")),\n",
        "        \"pearson\": float(true_series.corr(pred_series, method=\"pearson\")),\n",
        "        \"rmse\": float(np.sqrt(np.mean((y_true - y_pred) ** 2))),\n",
        "        \"mae\": float(np.mean(np.abs(y_true - y_pred))),\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def summarize_skip_table(skip_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if skip_df.empty:\n",
        "        return pd.DataFrame(columns=[\"reason\", \"count\"])\n",
        "    return (\n",
        "        skip_df.groupby(\"reason\")\n",
        "        .size()\n",
        "        .reset_index(name=\"count\")\n",
        "        .sort_values(\"count\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def apply_overrides(cfg_section: DictConfig, overrides: Dict[str, object]) -> None:\n",
        "    for key, value in overrides.items():\n",
        "        OmegaConf.update(cfg_section, key, value, force_add=True)\n",
        "\n",
        "\n",
        "def inspect_feature_availability(cfg: DictConfig, dataset_ids: Iterable[str]) -> pd.DataFrame:\n",
        "    records: List[Dict[str, object]] = []\n",
        "    for dms_id in dataset_ids:\n",
        "        record: Dict[str, object] = {\"DMS_id\": dms_id}\n",
        "        if cfg.kernel.use_sequence_kernel:\n",
        "            embedding_path = Path(cfg.data.paths.embeddings_singles) / f\"{dms_id}.h5\"\n",
        "            record[\"embeddings_exists\"] = embedding_path.exists()\n",
        "        else:\n",
        "            record[\"embeddings_exists\"] = True\n",
        "        if cfg.kernel.use_zero_shot:\n",
        "            zero_path = (\n",
        "                Path(cfg.data.paths.zero_shot)\n",
        "                / cfg.kernel.zero_shot_method\n",
        "                / f\"{dms_id}.csv\"\n",
        "            )\n",
        "            record[\"zero_shot_exists\"] = zero_path.exists()\n",
        "        else:\n",
        "            record[\"zero_shot_exists\"] = True\n",
        "        if cfg.kernel.use_structure_kernel:\n",
        "            cond_path = Path(cfg.data.paths.conditional_probs) / f\"{dms_id}.npy\"\n",
        "            record[\"conditional_probs_exists\"] = cond_path.exists()\n",
        "            use_distance = getattr(cfg.kernel.structure_kernel, \"use_distance_comparison\", False)\n",
        "            if use_distance:\n",
        "                coords_path = Path(cfg.data.paths.coords) / f\"{dms_id}.npy\"\n",
        "                record[\"coords_exists\"] = coords_path.exists()\n",
        "            else:\n",
        "                record[\"coords_exists\"] = True\n",
        "        records.append(record)\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "def clone_config(cfg: DictConfig) -> DictConfig:\n",
        "    return OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n",
        "\n",
        "\n",
        "def build_base_cfg(\n",
        "    kernel_config_name: str,\n",
        "    kernel_overrides: Dict[str, object],\n",
        "    optimization_overrides: Dict[str, object],\n",
        "    feature_paths: Dict[str, Optional[Path]],\n",
        "    reference_csv: Path,\n",
        "    assay_dir: Path,\n",
        "    output_dir: Path,\n",
        "    fold_column: str,\n",
        "    use_gpu: bool,\n",
        "    seed: int,\n",
        "    standardize: bool,\n",
        ") -> DictConfig:\n",
        "    cfg = OmegaConf.load(\"kermut/hydra_configs/benchmark.yaml\")\n",
        "    kernel_cfg_path = Path(\"kermut/hydra_configs/kernel\") / f\"{kernel_config_name}.yaml\"\n",
        "    if not kernel_cfg_path.exists():\n",
        "        raise FileNotFoundError(f\"Kernel config not found: {kernel_cfg_path}\")\n",
        "    cfg.kernel = OmegaConf.load(str(kernel_cfg_path))\n",
        "    apply_overrides(cfg.kernel, kernel_overrides)\n",
        "    cfg.dataset = \"single\"\n",
        "    cfg.cv_scheme = fold_column\n",
        "    cfg.single.use_id = True\n",
        "    cfg.single.id = None\n",
        "    cfg.use_gpu = bool(use_gpu)\n",
        "    cfg.overwrite = True\n",
        "    cfg.seed = int(seed)\n",
        "    cfg.progress_bar = False\n",
        "    cfg.data.paths.reference_file = str(reference_csv)\n",
        "    cfg.data.paths.DMS_input_folder = str(assay_dir)\n",
        "    cfg.data.paths.output_folder = str(output_dir)\n",
        "    if feature_paths.get(\"embeddings\") is not None:\n",
        "        cfg.data.paths.embeddings_singles = str(feature_paths[\"embeddings\"])\n",
        "    if feature_paths.get(\"zero_shot\") is not None:\n",
        "        cfg.data.paths.zero_shot = str(feature_paths[\"zero_shot\"])\n",
        "    if feature_paths.get(\"conditional_probs\") is not None:\n",
        "        cfg.data.paths.conditional_probs = str(feature_paths[\"conditional_probs\"])\n",
        "    if feature_paths.get(\"coords\") is not None:\n",
        "        cfg.data.paths.coords = str(feature_paths[\"coords\"])\n",
        "    cfg.data.standardize = bool(standardize)\n",
        "    cfg.data.test_index = -1\n",
        "    apply_overrides(cfg.optim, optimization_overrides)\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def run_kermut_cv(cfg: DictConfig, dms_id: str, target_seq: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    df, y, x_toks, x_embed, x_zero_shot = prepare_GP_inputs(cfg, dms_id)\n",
        "    gp_inputs = prepare_GP_kwargs(cfg, dms_id, target_seq)\n",
        "    df_out = df[[\"mutant\"]].copy()\n",
        "    df_out = df_out.assign(fold=np.nan, y=np.nan, y_pred=np.nan, y_var=np.nan)\n",
        "    metrics_records: List[Dict[str, object]] = []\n",
        "    unique_folds = sorted(df[cfg.cv_scheme].unique())\n",
        "    for test_fold in unique_folds:\n",
        "        torch.manual_seed(cfg.seed)\n",
        "        np.random.seed(cfg.seed)\n",
        "        test_mask = df[cfg.cv_scheme] == test_fold\n",
        "        train_mask = ~test_mask\n",
        "        train_idx = train_mask.tolist()\n",
        "        test_idx = test_mask.tolist()\n",
        "        y_train, y_test = split_inputs(train_idx, test_idx, y)\n",
        "        if cfg.data.standardize:\n",
        "            y_train, y_test = standardize(y_train, y_test)\n",
        "        x_toks_train, x_toks_test = split_inputs(train_idx, test_idx, x_toks)\n",
        "        x_embed_train, x_embed_test = split_inputs(train_idx, test_idx, x_embed)\n",
        "        x_zero_train, x_zero_test = split_inputs(train_idx, test_idx, x_zero_shot)\n",
        "        train_inputs = (x_toks_train, x_embed_train, x_zero_train)\n",
        "        test_inputs = (x_toks_test, x_embed_test, x_zero_test)\n",
        "        gp, likelihood = instantiate_gp(\n",
        "            cfg=cfg,\n",
        "            train_inputs=train_inputs,\n",
        "            train_targets=y_train,\n",
        "            gp_inputs=gp_inputs,\n",
        "        )\n",
        "        gp, likelihood = optimize_gp(\n",
        "            gp=gp,\n",
        "            likelihood=likelihood,\n",
        "            train_inputs=train_inputs,\n",
        "            train_targets=y_train,\n",
        "            lr=cfg.optim.lr,\n",
        "            n_steps=cfg.optim.n_steps,\n",
        "            progress_bar=cfg.optim.progress_bar,\n",
        "        )\n",
        "        df_out = predict(\n",
        "            gp=gp,\n",
        "            likelihood=likelihood,\n",
        "            test_inputs=test_inputs,\n",
        "            test_targets=y_test,\n",
        "            test_fold=int(test_fold),\n",
        "            test_idx=test_idx,\n",
        "            df_out=df_out,\n",
        "        )\n",
        "        test_count = int(test_mask.sum())\n",
        "        y_test_np = df_out.loc[test_idx, \"y\"].to_numpy(dtype=float)\n",
        "        y_pred_test_np = df_out.loc[test_idx, \"y_pred\"].to_numpy(dtype=float)\n",
        "        metrics_test = compute_regression_metrics(y_test_np, y_pred_test_np)\n",
        "        metrics_test.update({\"fold\": int(test_fold), \"partition\": \"test\", \"n\": test_count})\n",
        "        metrics_records.append(metrics_test)\n",
        "        train_inputs_filtered = tuple(x for x in train_inputs if x is not None)\n",
        "        if train_inputs_filtered:\n",
        "            with torch.no_grad():\n",
        "                train_dist = likelihood(gp(*train_inputs_filtered))\n",
        "                train_mean = train_dist.mean.detach().cpu().numpy()\n",
        "            y_train_np = y_train.detach().cpu().numpy()\n",
        "            metrics_train = compute_regression_metrics(y_train_np, train_mean)\n",
        "        else:\n",
        "            metrics_train = {\"spearman\": np.nan, \"pearson\": np.nan, \"rmse\": np.nan, \"mae\": np.nan}\n",
        "        train_count = int(train_mask.sum())\n",
        "        metrics_train.update({\"fold\": int(test_fold), \"partition\": \"train\", \"n\": train_count})\n",
        "        metrics_records.append(metrics_train)\n",
        "    df_out.rename(columns={\"fold\": \"test_fold\"}, inplace=True)\n",
        "    df_out[\"mutated_sequence\"] = df[\"mutated_sequence\"].values\n",
        "    df_out[cfg.cv_scheme] = df[cfg.cv_scheme].values\n",
        "    if \"DMS_score\" in df.columns:\n",
        "        df_out[\"DMS_score\"] = df[\"DMS_score\"].values\n",
        "    df_out[\"DMS_id\"] = dms_id\n",
        "    metrics_df = pd.DataFrame(metrics_records)\n",
        "    return df_out, metrics_df\n",
        "\n",
        "\n",
        "def summarize_cv_metrics(metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    metrics_cols = [\"spearman\", \"pearson\", \"rmse\", \"mae\"]\n",
        "    summary = (\n",
        "        metrics_df.groupby([\"DMS_id\", \"partition\"])[metrics_cols]\n",
        "        .agg([\"mean\", \"std\"])\n",
        "        .sort_index()\n",
        "    )\n",
        "    return summary\n",
        "\n",
        "\n",
        "def aggregate_prediction_metrics(cv_results: Dict[str, Dict[str, pd.DataFrame]]) -> pd.DataFrame:\n",
        "    records: List[Dict[str, object]] = []\n",
        "    for dms_id, payload in cv_results.items():\n",
        "        df_pred = payload[\"predictions\"].dropna(subset=[\"y_pred\"])\n",
        "        metrics = compute_regression_metrics(\n",
        "            df_pred[\"y\"].to_numpy(dtype=float), df_pred[\"y_pred\"].to_numpy(dtype=float)\n",
        "        )\n",
        "        metrics[\"DMS_id\"] = dms_id\n",
        "        metrics[\"n\"] = len(df_pred)\n",
        "        records.append(metrics)\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "def fit_full_model(cfg: DictConfig, dms_id: str, target_seq: str) -> pd.DataFrame:\n",
        "    df, y, x_toks, x_embed, x_zero_shot = prepare_GP_inputs(cfg, dms_id)\n",
        "    gp_inputs = prepare_GP_kwargs(cfg, dms_id, target_seq)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    np.random.seed(cfg.seed)\n",
        "    if cfg.data.standardize:\n",
        "        y_train, _ = standardize(y, y)\n",
        "    else:\n",
        "        y_train = y\n",
        "    train_inputs = (x_toks, x_embed, x_zero_shot)\n",
        "    gp, likelihood = instantiate_gp(\n",
        "        cfg=cfg,\n",
        "        train_inputs=train_inputs,\n",
        "        train_targets=y_train,\n",
        "        gp_inputs=gp_inputs,\n",
        "    )\n",
        "    gp, likelihood = optimize_gp(\n",
        "        gp=gp,\n",
        "        likelihood=likelihood,\n",
        "        train_inputs=train_inputs,\n",
        "        train_targets=y_train,\n",
        "        lr=cfg.optim.lr,\n",
        "        n_steps=cfg.optim.n_steps,\n",
        "        progress_bar=cfg.optim.progress_bar,\n",
        "    )\n",
        "    inputs_filtered = tuple(x for x in train_inputs if x is not None)\n",
        "    with torch.no_grad():\n",
        "        posterior = likelihood(gp(*inputs_filtered))\n",
        "        posterior_mean = posterior.mean.detach().cpu().numpy()\n",
        "        posterior_var = posterior.covariance_matrix.diag().detach().cpu().numpy()\n",
        "    df_out = df.copy()\n",
        "    df_out[\"posterior_mean\"] = posterior_mean\n",
        "    df_out[\"posterior_variance\"] = posterior_var\n",
        "    df_out[\"DMS_id\"] = dms_id\n",
        "    return df_out\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Kermut-ready assay files\n",
        "\n",
        "The next cell reads the TSV, filters single substitutions, aggregates the configured objectives, assigns three cross-validation folds, and saves one CSV per assay alongside a reference manifest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not tsv_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Input TSV not found at {tsv_path}. Update `tsv_path` in the configuration cell.\"\n",
        "    )\n",
        "\n",
        "df_raw = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "print(f\"Loaded {len(df_raw):,} rows from {tsv_path}\")\n",
        "\n",
        "objectives_df = validate_objective_config(objectives)\n",
        "required_columns = {\"reference_key\", \"sequence\", \"mutations\", \"backbone_id\", \"pdb\"}\n",
        "required_columns |= set(objectives_df[\"column\"])\n",
        "ensure_columns(df_raw, required_columns)\n",
        "\n",
        "df_single, skipped_rows = extract_single_mutants(df_raw, objectives_df)\n",
        "print(f\"Identified {len(df_single):,} candidate single mutants (skipped {len(skipped_rows):,} rows).\")\n",
        "\n",
        "if df_single.empty:\n",
        "    raise ValueError(\"No single mutants available after filtering. Check the `mutations` column.\")\n",
        "\n",
        "if DROP_ROWS_WITH_MISSING_OBJECTIVES:\n",
        "    df_single, dropped_missing = drop_rows_with_missing_objectives(df_single, objectives_df[\"column\"].tolist())\n",
        "    if not dropped_missing.empty:\n",
        "        print(f\"Dropped {len(dropped_missing):,} rows with missing objective values.\")\n",
        "else:\n",
        "    dropped_missing = pd.DataFrame(columns=df_single.columns)\n",
        "\n",
        "group_cols = [\"reference_key\", \"mutant\", \"mutation\", \"wt_aa\", \"mut_aa\", \"position\"]\n",
        "agg_dict = {col: \"mean\" for col in objectives_df[\"column\"]}\n",
        "agg_dict.update(\n",
        "    {\n",
        "        \"sequence\": \"first\",\n",
        "        \"mutated_sequence\": \"first\",\n",
        "        \"backbone_id\": \"first\",\n",
        "        \"pdb\": \"first\",\n",
        "    }\n",
        ")\n",
        "df_single = (\n",
        "    df_single.groupby(group_cols, dropna=False, as_index=False)\n",
        "    .agg(agg_dict)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "df_normalized, objective_stats = normalize_objectives(df_single, objectives_df, OBJECTIVE_NORMALIZATION)\n",
        "df_scored = compute_weighted_scores(df_normalized, objectives_df)\n",
        "\n",
        "assay_tables: Dict[str, pd.DataFrame] = {}\n",
        "fold_summaries: List[Dict[str, object]] = []\n",
        "for dms_id, group_df in df_scored.groupby(\"reference_key\"):\n",
        "    df_with_folds = assign_cv_folds(group_df.reset_index(drop=True), FOLD_COLUMN, CV_FOLDS, RANDOM_SEED)\n",
        "    assay_tables[dms_id] = df_with_folds\n",
        "    fold_counts = df_with_folds[FOLD_COLUMN].value_counts().sort_index()\n",
        "    fold_summary = {\"reference_key\": dms_id, \"n_variants\": int(len(df_with_folds))}\n",
        "    for fold_id, count in fold_counts.items():\n",
        "        fold_summary[f\"fold_{fold_id}\"] = int(count)\n",
        "    fold_summaries.append(fold_summary)\n",
        "\n",
        "processed_df = pd.concat(assay_tables.values(), ignore_index=True)\n",
        "reference_df = build_reference_table(df_scored)\n",
        "\n",
        "objective_config_path = workspace_root / \"objectives.json\"\n",
        "objective_config_path.write_text(json.dumps(objectives_df.to_dict(orient=\"records\"), indent=2))\n",
        "normalization_stats_path = workspace_root / \"objective_normalization.json\"\n",
        "normalization_stats_path.write_text(json.dumps(objective_stats, indent=2))\n",
        "\n",
        "for dms_id, table in assay_tables.items():\n",
        "    save_cols = [\"mutant\", \"mutated_sequence\", \"DMS_score\", FOLD_COLUMN] + objectives_df[\"column\"].tolist()\n",
        "    output_path = assay_output_dir / f\"{dms_id}.csv\"\n",
        "    table[save_cols].to_csv(output_path, index=False)\n",
        "\n",
        "reference_df.to_csv(reference_manifest_path, index=False)\n",
        "\n",
        "fold_summary_df = pd.DataFrame(fold_summaries).sort_values(\"reference_key\").reset_index(drop=True)\n",
        "skip_summary = summarize_skip_table(skipped_rows)\n",
        "\n",
        "print(f\"Prepared {len(assay_tables)} assays. Files written to {assay_output_dir}.\")\n",
        "print(f\"Reference manifest: {reference_manifest_path}\")\n",
        "\n",
        "processed_df.head()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Fold counts per assay:\")\n",
        "display(fold_summary_df)\n",
        "\n",
        "print(\"\\nSkipped or rejected rows:\")\n",
        "display(skip_summary)\n",
        "\n",
        "if 'dropped_missing' in locals() and not dropped_missing.empty:\n",
        "    print(\"\\nRows dropped because of missing objective values (showing first 5 rows):\")\n",
        "    display(dropped_missing.head())\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature preparation checklist\n",
        "\n",
        "The default `kermut` kernel expects:\n",
        "\n",
        "- **ESM2 embeddings** stored under `data/embeddings/substitutions_singles/ESM2/<DMS_id>.h5`.\n",
        "- **ProteinMPNN conditional probabilities** saved as `.npy` files inside `data/conditional_probs/ProteinMPNN/`.\n",
        "- **3D coordinates** (`.npy`) matching the same `<DMS_id>` in `data/structures/coords/` when distance comparisons are enabled.\n",
        "- Optional **zero-shot scores** (`data/zero_shot_fitness_predictions/<model>/<DMS_id>.csv`) if `use_zero_shot` is true.\n",
        "\n",
        "Generate these artefacts with the CLI utilities in `kermut/cmdline/preprocess_data`, for example:\n",
        "\n",
        "```bash\n",
        "python -m kermut.cmdline.preprocess_data.extract_esm2_embeddings data.paths.reference_file=<your_reference.csv> data.paths.DMS_input_folder=<assay_dir>\n",
        "python -m kermut.cmdline.preprocess_data.extract_ProteinMPNN_probs data.paths.reference_file=<your_reference.csv> data.paths.DMS_input_folder=<assay_dir>\n",
        "python -m kermut.cmdline.preprocess_data.extract_3d_coords data.paths.reference_file=<your_reference.csv>\n",
        "```\n",
        "\n",
        "Adjust the overrides so that the scripts pick up the manifest and assay directory generated above. Skip commands for features that you disable via `kernel_config_name` or `kernel_overrides`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compose Hydra config and verify feature availability\n",
        "\n",
        "Build a benchmark configuration programmatically so we can pass it to the reusable training utilities without spawning a new Hydra process. The feature availability table helps catch missing embeddings or structure files before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if reference_df.empty:\n",
        "    raise ValueError(\"Reference manifest is empty. Verify that the TSV produced at least one assay.\")\n",
        "\n",
        "base_cfg = build_base_cfg(\n",
        "    kernel_config_name=kernel_config_name,\n",
        "    kernel_overrides=kernel_overrides,\n",
        "    optimization_overrides=optimization_overrides,\n",
        "    feature_paths=feature_paths,\n",
        "    reference_csv=reference_manifest_path,\n",
        "    assay_dir=assay_output_dir,\n",
        "    output_dir=workspace_root / \"outputs\",\n",
        "    fold_column=FOLD_COLUMN,\n",
        "    use_gpu=USE_GPU,\n",
        "    seed=RANDOM_SEED,\n",
        "    standardize=DATA_STANDARDIZE,\n",
        ")\n",
        "\n",
        "print(f\"Kernel configuration: {base_cfg.kernel.name}\")\n",
        "feature_status = inspect_feature_availability(base_cfg, reference_df[\"DMS_id\"].tolist())\n",
        "display(feature_status)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run three-fold cross-validation\n",
        "\n",
        "For each assay the loop below trains Kermut on two folds, evaluates on the held-out fold, and records the metrics. Per-fold predictions and metrics are written to `cv_predictions/` inside the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_results: Dict[str, Dict[str, pd.DataFrame]] = {}\n",
        "metrics_tables: List[pd.DataFrame] = []\n",
        "\n",
        "for row in reference_df.itertuples(index=False):\n",
        "    dms_id = row.DMS_id\n",
        "    target_seq = row.target_seq\n",
        "    cfg = clone_config(base_cfg)\n",
        "    cfg.single.id = dms_id\n",
        "    print(f\"Running 3-fold CV for {dms_id} (length {len(target_seq)})\")\n",
        "    predictions_df, metrics_df = run_kermut_cv(cfg, dms_id, target_seq)\n",
        "    predictions_path = cv_predictions_dir / f\"{dms_id}_cv_predictions.csv\"\n",
        "    metrics_path = cv_predictions_dir / f\"{dms_id}_cv_metrics.csv\"\n",
        "    predictions_df.to_csv(predictions_path, index=False)\n",
        "    metrics_df.to_csv(metrics_path, index=False)\n",
        "    payload = {\"predictions\": predictions_df, \"metrics\": metrics_df}\n",
        "    cv_results[dms_id] = payload\n",
        "    metrics_with_id = metrics_df.copy()\n",
        "    metrics_with_id[\"DMS_id\"] = dms_id\n",
        "    metrics_tables.append(metrics_with_id)\n",
        "\n",
        "all_metrics_df = pd.concat(metrics_tables, ignore_index=True)\n",
        "metrics_summary = summarize_cv_metrics(all_metrics_df)\n",
        "metrics_summary\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summarise train/test performance across folds\n",
        "\n",
        "Flatten the per-fold metrics, save them for downstream reporting, and compute aggregate statistics for the test partitions as well as overall predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_summary_flat = metrics_summary.copy()\n",
        "metrics_summary_flat.columns = [\n",
        "    f\"{metric}_{stat}\" for metric, stat in metrics_summary_flat.columns.to_flat_index()\n",
        "]\n",
        "metrics_summary_flat = metrics_summary_flat.reset_index()\n",
        "display(metrics_summary_flat)\n",
        "\n",
        "all_metrics_df.to_csv(workspace_root / \"cv_metrics_folds.csv\", index=False)\n",
        "metrics_summary_flat.to_csv(workspace_root / \"cv_metrics_summary.csv\", index=False)\n",
        "\n",
        "overall_prediction_metrics = aggregate_prediction_metrics(cv_results)\n",
        "display(overall_prediction_metrics)\n",
        "overall_prediction_metrics.to_csv(\n",
        "    workspace_root / \"cv_metrics_overall_predictions.csv\", index=False\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: fit final models on the full dataset\n",
        "\n",
        "After inspecting cross-validation performance you can refit Kermut on the entire assay to obtain posterior means/variances for every single mutant. Uncomment and run the next cell if you want those predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# final_predictions_dir = workspace_root / \"final_model_predictions\"\n",
        "# final_predictions_dir.mkdir(parents=True, exist_ok=True)\n",
        "#\n",
        "# for row in reference_df.itertuples(index=False):\n",
        "#     dms_id = row.DMS_id\n",
        "#     cfg = clone_config(base_cfg)\n",
        "#     cfg.single.id = dms_id\n",
        "#     print(f\"Fitting final GP on all data for {dms_id}\")\n",
        "#     full_df = fit_full_model(cfg, dms_id, row.target_seq)\n",
        "#     full_df.to_csv(final_predictions_dir / f\"{dms_id}_posterior.csv\", index=False)\n",
        "# print(\"Saved posterior predictions for each assay.\")\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}